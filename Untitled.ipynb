{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как правильно подавать по частям данные для обучения ?\n",
    "\n",
    "- генератор\n",
    "- батчи\n",
    "\n",
    "Сколько и каких слоев надо ?\n",
    "\n",
    "Нужен ли TimeDestributed слой ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Нужна ли Statefull LSTM ?</b>\n",
    "<br>\n",
    "https://stackoverflow.com/questions/39681046/keras-stateful-vs-stateless-lstms\n",
    "<br>\n",
    "<i>As you can see above, when two sequences in two batches have connections (e.g. prices of one stock), you'd better use stateful mode, else (e.g. one sequence represents a complete sentence) you should use stateless mode.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>nb_samples, timesteps, input_dim Explanation</b>\n",
    "https://github.com/keras-team/keras/issues/834\n",
    "\n",
    "<br>\n",
    "Hey, I am currently working on a LSTM Music Composer.\n",
    "Looking at the required Input Shape of LSTM the Documentation states as follows: Input shape: 3D tensor with shape: (nb_samples, timesteps, input_dim).\n",
    "I was wondering what nb_samples, timesteps, and input_dim actually mean exactly. I know it must be obvious, but I couldn't figure out what nb_samples is exactly referring to.\n",
    "\n",
    "So I am going to feed my LSTM RNN with a piano roll like representation of a Midi File. So it's a matrix with ones and zeros, 1 meaning note_on, 0 meaning note_off. So what I have as piano roll is a matrix with each row representing one note (= input_dim?) and the columns representing one Midi tick (timing in midi files) (= timesteps?). This tells the Net which Note is on at what point in time. But I can't figure what the third dimension should be in my case.\n",
    "<br>\n",
    "\n",
    "<i>Assumedly you have more than a single MIDI file (otherwise you won't be learning much). So each MIDI file you have is a \"sample\", and the 0th dimension of your input tensor is your different files.<i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Reccurent input shape:</b>\n",
    "<br>\n",
    "3D tensor with shape (nb_samples, timesteps, input_dim)\n",
    "<br>\n",
    "<b>output shape</b>\n",
    "- if return_sequences: 3D tensor with shape  (nb_samples, timesteps, output_dim).\n",
    "- else, 2D tensor with shape (nb_samples, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note on using statefulness in RNNs</b>\n",
    "https://faroit.github.io/keras-docs/0.3.3/layers/recurrent/\n",
    "<br>\n",
    "To enable statefulness: - specify `stateful=True` in the layer constructor. - specify a fixed batch size for your model, by passing a `batch_input_shape=(...)` to the first layer in your model. This is the expected shape of your inputs including the batch size. It should be a tuple of integers, e.g. `(32, 10, 100)`.\n",
    "\n",
    "To reset the states of your model, call `.reset_states()` on either a specific layer, or on your entire model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Stateful shape</b>\n",
    "<br>\n",
    "http://philipperemy.github.io/keras-stateful-lstm/\n",
    "<br>\n",
    "I’m given a big sequence (e.g. Time Series) and I split it into smaller sequences to construct my input matrix X. Is it possible that the LSTM may find dependencies between the sequences? \n",
    "No it’s not possible unless you go for the stateful LSTM. Most of the problems can be solved with stateless LSTM so if you go for the stateful mode, make sure you really need it. In stateless mode, long term memory does not mean that the LSTM will remember the content of the previous batches.\n",
    "\n",
    "Why do we make the difference between stateless and stateful LSTM in Keras? \n",
    "A LSTM has cells and is therefore stateful by definition (not the same stateful meaning as used in Keras). Fabien Chollet gives this definition of statefulness: \n",
    "stateful: Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. \n",
    "\n",
    "Said differently, whenever you train or test your LSTM, you first have to build your input matrix X of shape nb_samples, timesteps, input_dim where your batch size divides nb_samples. For instance, if nb_samples=1024 and batch_size=64, it means that your model will receive blocks of 64 samples, compute each output (whatever the number of timesteps is for every sample), average the gradients and propagate it to update the parameters vector. \n",
    "\n",
    "By default, Keras shuffles (permutes) the samples in X and the dependencies between Xi and Xi+1 are lost. Let’s assume there’s no shuffling in our explanation. \n",
    "\n",
    "If the model is stateless, the cell states are reset at each sequence. With the stateful model, all the states are propagated to the next batch. It means that the state of the sample located at index i, Xi will be used in the computation of the sample Xi+bs in the next batch, where bs is the batch size (no shuffling).\n",
    "\n",
    "Why do Keras require the batch size in stateful mode? \n",
    "When the model is stateless, Keras allocates an array for the states of size output_dim (understand number of cells in your LSTM). At each sequence processing, this state array is reset. \n",
    "\n",
    "In Stateful model, Keras must propagate the previous states for each sample across the batches. Referring to the explanation above, a sample at index i in batch #1 (Xi+bs) will know the states of the sample i in batch #0 (Xi). In this case, the structure to store the states is of the shape (batch_size, output_dim). This is the reason why you have to specify the batch size at the creation of the LSTM. If you don’t do so, Keras may raise an error to remind you: If a RNN is stateful, a complete input_shape must be provided (including batch size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
